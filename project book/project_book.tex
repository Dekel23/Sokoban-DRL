\documentclass[12pt,a4paper]{report}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{titlesec}   % For customizing chapter titles
\usepackage{hyperref}   % For hyperlinks in the table of contents
\usepackage{newtxtext,newtxmath}  % Times New Roman font for text and math
\usepackage{amsmath}

\geometry{
    a4paper,
    left=3cm,
    right=3cm,
    top=1cm,
    bottom=3cm,
}

% Custom commands for title page information
\newcommand{\projectname}{Formal Verification Methods for Solving Spatial Games}
\newcommand{\firstauthor}{Boaz Gurevich}
\newcommand{\secondauthor}{Erel Dekel}
\newcommand{\firstsupervisor}{Hillel Kogler}
\newcommand{\secondsupervisor}{Avraham Raviv}
\newcommand{\submissiondate}{Thursday, October 10th 2024}

% Redefine the title page
\renewcommand{\maketitle}{
    \begin{titlepage}
        \centering
        \vspace*{2cm}
        
        {\huge\bfseries\projectname\par}
        \vspace{1.5cm}
        
        {\large\textbf{Authors:}\par}
        \vspace{0.5cm}
        {\large\firstauthor\hspace{0.3cm}\textit{\&}\hspace{0.3cm}\secondauthor\par}
        
        \vspace{2cm}
        
        {\large\textbf{Supervisors:}\par}
        \vspace{0.5cm}
        {\large\firstsupervisor\hspace{0.3cm}\textit{\&}\hspace{0.3cm}\secondsupervisor\par}
        
        \vspace{2cm}

        {\large\textbf{Date:}\par}
        \vspace{0.5cm}
        {\large\submissiondate\par}
    \end{titlepage}
}

% Customizing the chapter title format to remove "Chapter X"
\titleformat{\chapter}[hang]{\LARGE\bfseries}{\thechapter.}{1em}{}

% Redefine the \section size
\titleformat{\section}
    {\normalfont\large\bfseries} % This sets \section titles to \large
    {\thesection}{1em}{}

% Redefine the \subsection size and numbering
\titleformat{\subsection}
    {\normalfont\normalsize\bfseries} % This sets \subsection titles to \normalsize
    {\thesubsection}{1em}{}

% Redefine the \subsubsection size and numbering (optional)
\titleformat{\subsubsection}
    {\normalfont\normalsize\bfseries} % This sets \subsubsection titles to \normalsize
    {\thesubsubsection}{1em}{}


% Adjust line spacing
\linespread{1.5}

\begin{document}

% Enable hyperlinks in the table of contents
\hypersetup{
    linkcolor=blue,
    urlcolor=blue,
    linktoc=all
}

% Generate the title page
\maketitle

\chapter*{Abstract}
Reinforcement Learning (RL) has emerged as a cornerstone of modern technology, driving advancements in areas such as autonomous systems, robotics, and adaptive control.\\
However, a critical challenge in deploying RL algorithms lies in their tendency to suffer from convergence issues, particularly in complex, high-dimensional environments where instability can lead to suboptimal or unsafe policies.\\
This project aims to explore the impact of formal verification techniques on RL, specifically focusing on how these methods can address the convergence challenges that plague RL systems.\\
We will empirically investigate this by applying formal verification to a model-free RL algorithm designed to solve the Sokoban puzzle game, a notoriously difficult problem requiring complex planning and decision-making.\\

\tableofcontents

\chapter{Background}
In this chapter, we introduce the fundamental concepts that critical for the reader to understand.
We will provide deep understanding of Neural Networks (NNs), Reinforcement Learning (RL), Deep Q-Learning (DQL), and Fomal Verification.
\section{Neural Networks}
Neural Networks (NNs) are machine learning models inspired by the brain structure which used for approximating complex functions. They consist of layers of interconnected neurons (nodes) through which information flows to learn how to map input data to outputs.
\subsection{Neural Network Structure}
A neural network is devided into 3 types of layers:
\begin{enumerate}
    \item \textbf{Input Layer:} Represents the input features of the network.
    \item \textbf{Hidden Layers:} Layers between the input and output layers, where most of the computations and transformation of the features happens.
    \item \textbf{Output Layer:} Represents the output of the network. Usually represent a classification probability vector.
\end{enumerate}
Each adjacent layers are connected weights and bias. Each neuron process information using a activation function, which adds non-linearity to the model which allows it to approximate more complex functions.
\subsection{TBA}
TBA
\section{Reinforcement Learning}
Reinforcement Learning is a type of machine learning where an agent learns to make decision (actions) based on interactions with the environment.\\
Unlike supervised and unsupervised learning, reinforcement learning doesn't learn through given training dataset but through the feedback that agent get when interacting with the environment which come in the form of rewards. Therefore, reinforcement learning ofter considered as separate paradigm of machine learning that focuses on decision-making and sequential actions in an environment.
\subsection{Markov Decision Process (MDP)}
MDPs provide a mathematical description of the environment, defined as the following tuple $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle$, where $\mathcal{S}$ is the set of all possible states, $\mathcal{A}$ is the set of all possible actions, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the probability transition function which defines the probability of transitioning to state $s_{t+1}$ taking action $a_t$ from state $s_t$, $\mathcal{R}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the reward function which gives the expected reward for taking action $a_t$ at state $s_t$, and $\gamma\in[0,1)$ is the discout factor.\\
In MDPs $\mathcal{P}(s_{t+1}|s_t,a_t)=\mathcal{P}(s_{t+1}|s_t,a_t,\dots,s_0,a_0)$ which means that the following state depends only on the current state and action and not on the past states and actions. In reinforcement learning the environment often can be modeled as an MDP.
\subsection{Key Componants of Reinforcement Learning}
\begin{itemize}
    \item \textbf{Agent:} Interacts with the environment and decides which action to take in each state.
    \item \textbf{Environment:} Responds to the agent's actions and provide the new states, usually can be modeled by a probability transition function $\mathcal{P}(s'|s,a)$.
    \item \textbf{State $s{\in}S$:} The current situation of the environment. The state space, $\mathcal{S}$, is the set of all possible states.
    \item \textbf{Action $a{\in}A$:} The decision the agent made in a given state. The action space, $\mathcal{A}$, is the set of all possible actions the agent can take, potentially depending on the current state.
    \item \textbf{Policy $\pi(a|s)$:} A mapping from states to probabilities of selecting each possible action. A policy can be deterministic ($\pi(s)=a$) or stochastic ($\pi(a|s)$ gives a probability distribution over actions).
    \item \textbf{Reward $R(s,a)$:} The immidiate reutrn value after preforming action $a$ in state $s$. A function the maps pairs of action and state to a real scalar $R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$.
    \item \textbf{Value Function $V^{\pi}(s)$:} Function $V:\mathcal{S}\to\mathbb{R}$ estimates the cumulative reward (future reward) starting from state $s$ and following policy $\pi$.
    \item \textbf{Q-Function $Q^{\pi}(s,a)$:} Also know as Action-Value Function. Function $\smash{Q:\mathcal{S}\times\mathcal{A}\to\mathbb{R}}$ estimates the cumulative reward starting from state $s$, taking action $a$, and following policy $\pi$.
\end{itemize}
\subsection{Types of Reinforcement Learning}
There are 2 primary types of reinforcement learning approaches:
\begin{enumerate}
    \item[] \textbf{Model-Free:} In this approach, the agent does not have a model of the environment but learn only from interactions with the environment.\\
    There are 2 main methods of making a model-free RL:
    \begin{itemize}
        \item \textbf{Value Based:} The agent learns a value function which estimates the value of the expected reward in a given state and/or taking an action.
        \item \textbf{Policy Based:} The agent learn a policy that maps states to actions without learning a value function. 
    \end{itemize}
    \item[] \textbf{Model-Based:} In this approach, the agent builds a model that mimics the environment and tries to predict the next state given the current state and action. Model-Based are more data efficient but more complex to build.
\end{enumerate}
\subsection{The Reinforcement Learning Problem}
On each step $t$, the agent observe a state $s_t\in\mathcal{S}$ in the environment, selects an action $a_t\in\mathcal{A}(s_t)$ according to its policy $\pi$ an recives a reward $r_{t+1}$, and the environment transition to a new state $s_{t+1}$. The environment dont have to be deterministic which mean the next state can be difined by the probability function $P(s_{t+1}|s_t,a_t)$.\\
The goal of the agent is to maximaze the future reward, also known as return. At time $t$ looking forward T steps in the future, and considering a discout factor $\gamma{\in}[0,1)$ (to ensure convergence as $T{\rightarrow}\infty$) one can represent the return as: $$G_t = r_{t+1} + {\gamma}r_{t+2}+\dots+{\gamma}^{T-1}r_{t+T}=\sum_{k=0}^{T-1}\gamma^{k}r_{t+k+1}$$
Notice earlier description of the Value Function and Q-Function can be expressed as: $$\begin{aligned}V^{\pi}(s)=\mathbb{E}_\pi[G_t|s_t=s] \quad Q^{\pi}(s,a)=\mathbb{E}_\pi[G_t|s_t=s, a_t=a]\end{aligned}$$
The objective is to find a policy $\pi^*$ which maximize the expected return form any state $s$: $${\pi}^*=\arg\max_{\pi}\mathbb{E}_\pi[G_t|s_t=s]$$
\subsection{Exploration vs. Exploitation}
One main challenge in reinforcement learning is the trade-off between exploration and exploitation.
\begin{itemize}
    \item \textbf{Exploration:} The agent tries new actions in order to discover better rewards and avoid getting stuck in suboptimal policy.
    \item \textbf{Exploitation:} The agent select actions the based on his current knowledge yeilds the highest reward.
\end{itemize}
Both of them are crucial for the success of the agent. One strategy of balancing them is called $\epsilon$-greedy which the agent explore each step at a probability of $\epsilon$.  
\section{Deep Q-Learning}
TBA
\section{Formal Verification}
TBA
\end{document}
